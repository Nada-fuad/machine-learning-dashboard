hydra:
  run:
    dir: ${output_dir}/${experiment_id}/${model.name}/
  sweep:
    dir: ${output_dir}/${experiment_id}/
    subdir: ${model.name}
  launcher:
    _target_: hydra._internal.core_plugins.basic_launcher.BasicLauncher
  sweeper:
    _target_: hydra._internal.core_plugins.basic_sweeper.BasicSweeper
    max_batch_size: null
    params:
      model: timm_resnet18,timm_mobilenetv3_small_075
      +model.pretrained: 'True'
  help:
    app_name: ${hydra.job.name}
    header: '${hydra.help.app_name} is powered by Hydra.

      '
    footer: 'Powered by Hydra (https://hydra.cc)

      Use --hydra-help to view Hydra specific help

      '
    template: '${hydra.help.header}

      == Configuration groups ==

      Compose your configuration from those groups (group=option)


      $APP_CONFIG_GROUPS


      == Config ==

      Override anything in the config (foo.bar=value)


      $CONFIG


      ${hydra.help.footer}

      '
  hydra_help:
    template: 'Hydra (${hydra.runtime.version})

      See https://hydra.cc for more info.


      == Flags ==

      $FLAGS_HELP


      == Configuration groups ==

      Compose your configuration from those groups (For example, append hydra/job_logging=disabled
      to command line)


      $HYDRA_CONFIG_GROUPS


      Use ''--cfg hydra'' to Show the Hydra config.

      '
    hydra_help: ???
  hydra_logging:
    version: 1
    formatters:
      simple:
        format: '[%(asctime)s][HYDRA] %(message)s'
    handlers:
      console:
        class: logging.StreamHandler
        formatter: simple
        stream: ext://sys.stdout
    root:
      level: INFO
      handlers:
      - console
    loggers:
      logging_example:
        level: DEBUG
    disable_existing_loggers: false
  job_logging:
    version: 1
    formatters:
      simple:
        format: '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'
      colorlog:
        (): colorlog.ColoredFormatter
        format: '[%(cyan)s%(asctime)s%(reset)s][%(blue)s%(name)s%(reset)s][%(log_color)s%(levelname)s%(reset)s]
          - %(message)s'
        log_colors:
          DEBUG: purple
          INFO: green
          WARNING: yellow
          ERROR: red
          CRITICAL: red
    handlers:
      console:
        class: logging.StreamHandler
        formatter: colorlog
        stream: ext://sys.stdout
      file:
        class: logging.FileHandler
        formatter: simple
        filename: ${hydra.job.name}.log
    root:
      level: INFO
      handlers:
      - console
      - file
    loggers:
      automate:
        level: WARNING
      numba:
        level: WARNING
      te_compiler:
        level: WARNING
    disable_existing_loggers: false
  env: {}
  mode: MULTIRUN
  searchpath: []
  callbacks: {}
  output_subdir: .hydra
  overrides:
    hydra: []
    task:
    - +experiment=pretrained_models
  job:
    name: train
    chdir: true
    override_dirname: +experiment=pretrained_models
    id: ???
    num: ???
    config_name: config
    env_set: {}
    env_copy: []
    config:
      override_dirname:
        kv_sep: '='
        item_sep: ','
        exclude_keys: []
  runtime:
    version: 1.3.2
    version_base: '1.2'
    cwd: /local/gerum/hannah/experiments/cifar10
    config_sources:
    - path: .
      schema: file
      provider: hannah
    - path: hydra.conf
      schema: pkg
      provider: hydra
    - path: hannah.conf
      schema: pkg
      provider: main
    - path: hydra_plugins.hydra_colorlog.conf
      schema: pkg
      provider: hydra-colorlog
    - path: hannah.conf
      schema: pkg
      provider: hannah
    - path: ''
      schema: structured
      provider: schema
    output_dir: ???
    choices:
      experiment: pretrained_models
      pseudo_labeling: null
      nas: null
      fine_tuning: null
      augmentation: cifar_augment
      compression: null
      profiler: null
      early_stopping: null
      backend: null
      checkpoint: default
      trainer: default
      module: image_classifier
      normalizer: null
      optimizer: sgd
      scheduler: 1cycle
      model: timm_resnet18
      features: identity
      unlabeled_data: null
      dataset: cifar10
      dataset/features: cifar10_identity
      hydra/env: default
      hydra/callbacks: null
      hydra/job_logging: silent
      hydra/hydra_logging: default
      hydra/hydra_help: default
      hydra/help: default
      hydra/sweeper: basic
      hydra/launcher: basic
      hydra/output: default
  verbose: false
dataset:
  data_folder: ${hydra:runtime.cwd}/datasets/
  cls: hannah.datasets.vision.Cifar10Dataset
  dataset: cifar10
  val_percent: 0.1
features:
  _target_: torch.nn.Identity
model:
  _target_: hannah.models.timm.TimmModel
  name: resnet18
scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  max_lr: ${optimizer.lr}
  pct_start: 0.3
  anneal_strategy: cos
  cycle_momentum: true
  base_momentum: 0.85
  max_momentum: 0.95
  div_factor: 25.0
  final_div_factor: 10000.0
  last_epoch: -1
optimizer:
  _target_: torch.optim.sgd.SGD
  lr: 0.3
  momentum: 0.9
  dampening: 0
  weight_decay: 0.0005
  nesterov: false
module:
  _target_: hannah.modules.vision.ImageClassifierModule
  num_workers: 0
  batch_size: 2048
  shuffle_all_dataloaders: false
trainer:
  _target_: pytorch_lightning.trainer.Trainer
  accelerator: auto
  devices: 1
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  max_epochs: 50
  default_root_dir: .
  fast_dev_run: false
  overfit_batches: 0.0
  benchmark: false
  deterministic: warn
  gradient_clip_val: 0
  accumulate_grad_batches: 1
  plugins: null
  strategy: auto
  reload_dataloaders_every_n_epochs: 0
  precision: 16
checkpoint:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  dirpath: checkpoints
  save_top_k: 1
  verbose: true
  monitor: val_error
  mode: min
  save_last: true
augmentation:
  batch_augment:
    pipeline: null
    transforms:
      RandomHorizontalFlip:
        p: 0.5
      RandomAffine:
        degrees:
        - -15
        - 15
        translate:
        - 0.1
        - 0.1
        scale:
        - 0.9
        - 1.1
        shear:
        - -5
        - 5
        p: 0.5
      RandomCrop:
        size:
        - 32
        - 32
        padding: 4
      RandomErasing:
        p: 0.5
experiment_id: pretrained_models
output_dir: trained_models
auto_lr: false
resume: false
fx_mac_summary: false
skip_test: false
skip_val: false
seed:
- 1234
validate_output: false
monitor:
  metric: val_accuracy
  direction: maximize
